目前比较火的就是人工智能，更具体的说是深度学习，其中有一个比较重要的模型是卷积神经网络，为什么叫神经网络，这是因为深度学习这项技术最早是受到人脑的神经元的启发（当然最新的研究已经与神经元没有任何的关系了），所以我们就从神经元入手，来一路的感受一下深度学习这门技术的发展。

## 神经元的数学模型

人脑的神经系统是由一个个神经元互相连接而形成，一个神经元可以有多个输入，也可以同时输出到多个神经元。我们对神经元进行一个简单的建模

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/深度学习.jqc34h9jy34.webp" style="zoom: 50%;">

其数学模型如下所示：

$$
y = w_{1}x_{2} + w_{2}x_{2} + \cdots + w_{n}x_{n} = \sum_{i = 1}^{n}w_{i}x_{i}
$$

这里神经元对各个输入只是进行简单的加权和。

通过将神经元按照层级连接起来，就形成了神经网络，称为多层感知机（在卷积神经网络中又被称为全连接层）。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/多层感知机.6adzaqnibpg0.webp" style="zoom: 50%;" />

我们可以得到每层的输出与输入的关系

$$
\begin{cases}
y_{1} = w_{11}x_{1} + w_{12}x_{2} + \cdots + w_{1n}x_{n} \\
y_{2} = w_{21}x_{1} + w_{22}x_{2} + \cdots + w_{2n}x_{n}  \\
\vdots \\
y_{m} = w_{m1}x_{1} + w_{m2}x_{2} + \cdots + w_{mn}x_{n}  \\
\end{cases}
$$

为了简便，可以将其写为矩阵的形式

$$
y = Wx
$$

其中 

$$
y = \begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{m}
\end{bmatrix} \quad
W = \begin{bmatrix}
w_{11} &w_{12} &\cdots &w_{1n} \\
w_{21} &w_{22} &\cdots &w_{2n} \\
\vdots &\vdots &\cdots &\vdots \\
w_{m1} &w_{m2} &\cdots &w_{mn}
\end{bmatrix} \quad
x = \begin{bmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{bmatrix}
$$

一般我们将多层感知机分为三层，输入层、隐藏层与输出层，这里隐藏层只画了一层，但是隐藏层是具有若干层的。但是这个模型是一个线性模型，中间的多个隐藏层与一个无异，假设存在 $N$ 个隐藏层，那么输出与输入的关系为

$$
y = W_{N}W_{N - 1}W_{N - 2}\cdots W_{1}x
$$

我们令 $W=W_{N}W_{N - 1}W_{N - 2}\cdots W_{1}$，那么这 $N$ 个隐藏层等价于一个参数为 $W$ 隐藏层，增加层数并不会使得神经网络的表达能力得到提升。

根本原因还是这个模型是一个线性的模型，多个线性模型连接得到的还是一个线性模型。还是考虑到人类的神经元，当其受到刺激时（接受输入），只有当输入达到一定阈值时，神经元才会被激活从而进行输出，基于这一启发，我们需要为每层的网络后添加一个激活函数，即

$$
y = \sigma(Wx)
$$

其中 $\sigma$ 就是激活函数，目前有各种各样的激活函数，常见的有 `Sigmoid`、`tanh`、`ReLU`及`LeakyReLU` 等等，它们的函数形状如下所示。

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
	<div>
		<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/sigmoid.6h7xyw8yark0.webp" />
		<div style="text-align: center;">Sigmoid</div>
	</div>
	<div>
		<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/tanh.6bhk9p6dhjk0.webp" />
		<div style="text-align: center;">tanh</div>
	</div>
	<div>
		<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/ReLU.4i6xjp7bjeo0.webp" />
		<div style="text-align: center;">ReLU</div>
	</div>
	<div>
		<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/LeakyReLU.8wws5bozj48.webp" />
		<div style="text-align: center;">LeakyReLU</div>
	</div>
</div>

通过为每层添加激活函数从而使得模型具有了非线性的能力

$$
y = \sigma_{N}(W_{N}\sigma_{N - 1}(W_{N - 1} \cdots \sigma_{1}(W_{1}x)))
$$

有理论能证明该模型能够逼近任意的函数。

## 训练流程

我们要明确我们训练模型训练的是什么，训练的是每层的参数 $W$，当训练完成之后，当我们输入 $x$ 时，能够得到希望的 $y$。那么如何训练呢，首先我们需要准备好数据集 $(x_{i}, y_{i})$，其中 $x_{i}$ 是第 $i$ 个输入，$y_{i}$ 表示我们我们期望的输出，或者说是学习的目标，而模型的真实输出为 $\hat{y}_{i}$，我们希望最终 $\hat{y}_{i}$ 能无限的逼近于 $y_{i}$。

在训练时，我们需要时刻比较 $\hat{y_{i}}$ 与 $y_{i}$ 的误差，根据误差，我们来调整参数 $W$，如此不断反复，直至模型收敛。那么就带来了两个问题：

1. 如何衡量 $\hat{y_{i}}$ 与 $y_{i}$ 的误差
2. 如何根据误差来调整参数 $W$

衡量 $\hat{y_{i}}$ 与 $y_{i}$ 的误差的东西我们称为目标函数（损失函数），不同的任务有不同的损失函数，一般回归任务使用均方误差损失函数

$$
l = \frac{1}{N}\sum_{i}^{N} (y_i - \hat{y}_i)^{2}
$$

而对于分类任务一般使用交叉熵损失函数

$$
l = -\frac{1}{N} \sum_i^N \sum_{c}^{M} y_{ic}\log(p_{ic})
$$

我们的目标是希望损失函数最小，这种最大最小某个函数的问题在数学上一般称为优化问题，而如何调整参数使得函数最优的算法称为优化算法。目前也有很多的优化算法，比如随机梯度下降算法，`Adam` 优化算法等等。

梯度下降算法是最基础的算法，考虑 $y=x^2$ 这一损失函数，现在我们位于该函数的随机的一点 $x_{0}$，当 $x_{0} < 0$ 时，我们希望 $x_{0}$ 增大，当 $x_{0} > 0$ 时，我们希望 $x_{0}$ 减小。我们发现我们始终是沿着梯度的反方向变化的，因此参数的迭代公式为

$$
x_{i + 1} = x_{i} - \eta \frac{\partial{y}}{\partial{x}}
$$

其中 $\eta$ 为学习率，学习率越大，学习的就越快，但是大的学习率会使得模型始终在震荡，模型不收敛，但是较小的学习率会使得模型收敛速度过慢，要训练很久。所以学习率的大小选择需要不断地尝试，像学习率这种认为选择的参数我们称为超参数，深度学习一个重要的过程就是选择合适的超参数，简称调参。

现在又带来了一个新的问题，怎么获得梯度，对于简单的函数梯度很容易算出来，但是对于复杂的神经网络，计算每个参数的梯度，几乎无法通过数学手动推导，这个问题可以通过反向传播算法解决，当我们构建模型时，除了会构建前向的计算流程，还会构建一个反向传播的计算图，用以传递梯度，这个过程由深度学习框架提供，我们无需关心。

整个的训练流程如下所示

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/网络训练流程.3lhliphwyfo0.webp" style="zoom: 50%;" />

## 手写数字识别

```python
import torch
from torch import nn
import torchvision
import matplotlib.pyplot as plt

num_epoches = 3
batch_size = 256
lr = 0.1

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

train_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST('./dataset/', train=True, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST('./dataset/', train=False, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size, shuffle=True)

for features, labels in train_loader:
  print(features.shape)
  print(labels.shape)
  break

net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))
net = net.to(device)
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

net.eval()
accuracy = []
for features, labels in test_loader:
  features = features.to(device)
  labels = labels.to(device)
  predict = net(features)
  acc = predict.argmax(axis=1).type(labels.dtype) == labels
  accuracy.append(acc.type(labels.dtype).sum() / len(labels))

print(torch.tensor(accuracy).mean())

net.train()
for epoch in range(num_epoches):
  i = 0
  for features, labels in train_loader:
    i = i + 1
    features = features.to(device)
    labels = labels.to(device)
    y_hat = net(features)
    l = loss(y_hat, labels)
    trainer.zero_grad()
    l.mean().backward()
    trainer.step()
    if i % 10 == 0:
      print(f'epoch: {epoch}, {i * len(labels)} / 60000, loss: {l.mean()}')

net.eval()
accuracy = []
for features, labels in test_loader:
  features = features.to(device)
  labels = labels.to(device)
  predict = net(features)
  acc = predict.argmax(axis=1).type(labels.dtype) == labels
  accuracy.append(acc.type(labels.dtype).sum() / len(labels))

print(torch.tensor(accuracy).mean())
```

## 卷积神经网络

如果用多层感知机来处理图像，会丢失图像的空间信息，因为我们是把图像展开为一维的向量作为输入的，另一个更严重的问题是，多层感知机无法处理大图像，因为所需要的资源实在是太多了。

考虑使用多层感知机来处理一幅 $1000\times1000$ 大小的图片，也就意味着输入层的节点数目为 $10^6$ 个，假设只有一个隐藏层，隐藏层的节点数目与输入节点的数目一样多，也为 $10^6$ 个，那么这个网络的参数个数为 $10^{12}$，这是一个惊人的数字，即使是在如今的计算机上也很难将其全部搬入到内存中，因此也无法展开训练。训练参数过于巨大，带来的训练时间以及所需内存随着层级的增多急剧增加，使得只能训练简单几层的神经网络，无法处理图像等复杂的应用问题。

根据图像具有的两个特征：

1. 平移不变性：一个物体出现在图片中的不同位置，网络应该有相同的输出，与位置无关
2. 局部性：图像中的像素往往只与周围的像素相关，与其较远的像素不相关

我们对多层感知机变换为二维的形式

$$
y_{i, j} = \sum_{k,l}w_{i,j,k,l}x_{k,l} = \sum_{a,b}v_{i,j,a,b}x_{i+a,j+b}
$$

应用平移不变性，即不管输入如何变化，权重 $v$ 不应该发生变化，即 $v_{i,j,a,b} = v_{a,b}$

$$
y_{i,j} = \sum_{a, b}v_{a,b}x_{i+a, j+b}
$$

接着应用图片的局部性，我们一次只看一部分，即对于 $|a| > \Delta, |b| > \Delta$，有 $v_{a, b} = 0$，所以有

$$
y_{i, j} = \sum_{a = -\Delta}^{\Delta}\sum_{b = -\Delta}^{\Delta}v_{a, b}x_{i + a, j + b}
$$

```info
其实在数学上，这个运算是相关运算，不是卷积运算，我估计是在图像处理领域，以往的做法都是使用滤波器处理图像，而滤波器牵涉到卷积运算，大家叫惯了，所以叫卷积神经网络。
```

<img src="https://zh.d2l.ai/_images/conv-multi-in.svg" />

由于卷积操作会导致卷积层的输出的特征变小，为了保持特征的大小，我们可以对于输入特征进行填充，在卷积的过程中，步幅的大小也会输出特征的特征产生影响，步幅越大，输出的特征越小。对于 $k_w \times k_h$ 大小的卷积核，填充为 $p_w \times p_h$，步幅为 $s_w \times s_h$，输入的特征大小为 $w \times h$，那么输出的特征大小为：

$$
\frac{w - k_w + p_w + s_w}{s_w} \times \frac{h - k_h + p_h + s_h}{s_{h}}
$$

在 `pytorch` 中，通过 `nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)` 可以快速构建一个卷积层

```python
nn.Conv2d(1, 6, kernel_size=5, padding=2)
```

池化（汇聚）操作是为了提高系统的鲁棒性，为了减小图像的微小变化而对网络的输出产生大的影响，我们一般在卷积操作之后进行池化操作，池化分为两种，最大池化和平均池化。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/池化层.37f73to0q4u0.webp" style="zoom: 50%;" />

```python
# 平均池化
nn.AvgPool2d(kernel_size=2, stride=2)
# 最大池化
nn.MaxPool2d(kernel_size=2, stride=2)
```

```info
池化层在现在的网络中已经应用的比较少了，在之前的卷积神经网络中出现的比较多。
```

## 经典的卷积神经网络

LeNet 是最早发表的卷积神经网络，它是由 Yann LeCun 于 1989 年发表的，当时的目的是为了识别图像中的数字。网络整体结构由卷积层和全连接层组成，LeNet 有两个卷积层，卷积核的大小都为 $5\times5$，激活函数使用的都是 Sigmoid 函数，池化层采取的都是步幅为 $2$ 的平均池化，第一个卷积层对原图像进行了填充，以保持输出的特征大小不发生改变。在网络最后使用了三个全连接层，分别有 $120$、$84$ 和 $10$ 个输出，最后的 $10$ 个输出分别表示 $10$ 个数字的置信度。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/毕业论文-LeNet.3jcaxdbo02k0.webp" style="zoom: 50%;" />

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

当时的计算机的计算能力还比较弱，这个系统是通过专用的硬件实现的，因此没有实现大规模的应用。随着计算能力的不断提升，卷积神经网络被研究者们重新发掘，在 2012 年的 ImageNet 图片分类的比赛中，卷积神经网络 AlexNet 大放异彩，一举拿下比赛的冠军，并且远超第二名的精度，从此卷积神经网络成为机器学习的主流。

AlexNet 使用八层卷积神经网络，它包含 5 个卷积层以及 3 个全连接层，其网络架构如图所示。AlexNet 相对于 LeNet 深度有所提升，并且使用的激活函数为 ReLU 函数。第一个卷积层的卷积核的大小为         $11 \times 11$，后面卷积层的卷积核依次递减为 $5 \times 5$ 和 $3 \times 3$，在在第一层、第二层和第五层后面加上了大小为 $3 \times 3$，步幅为 $2$ 的最大池化层。在网络的后部分使用了三个全连接层，前两个全连接层的输出为 $4096$，最后一个全连接层的输出为 $1000$，也是 ImageNet 比赛中的图片的总类别数。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/毕业论文-AlexNet.43wqiryxroo.webp" style="zoom: 50%;" />

```python
net = nn.Sequential(
    # 这里，我们使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))
```

VGG 首次提出了块的概念，通过连接块来组成一个网络。VGG 的块由若干个卷积层和一个最大池化层组成。VGG 块使用若干个填充为 $1$ 的 $3 \times 3$ 的卷积层，填充为 $1$ 可以保持输入输出的特征的高宽不会发生改变。通过组合若干个 VGG 块，然后再末尾连接若干全连接层，便形成了 VGG 网络。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/毕业论文-VGG.1ro5rbog8lr4.webp" style="zoom: 50%;" />

```python
def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)
```

GoogleNet 首次提出并行网络结构，在以往的研究中，卷积层应该使用多大的卷积核，大家没有一个统一的标准，小到 $1 \times 1$，大到 $11 \times 11$。GoogleNet 提出了一个新的想法，通过组合不同大小的卷积核可能是最有效的。在 GoogleNet 中最核心的是被称为 Inception 的块，它由四个并行模块组成。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/毕业论文-GoogleNet.k7yl763xddo.webp" style="zoom: 50%;" />

前三条路径使用 $1 \times 1$、$3 \times 3$ 和 $5 \times 5$ 的卷积来提取图像中的特征，第二条路径和第三条路径首先使用了 $1 \times 1$ 的卷积层，来减少通道数，减少模型的复杂度；第四条路径使用了 $3 \times 3$ 最大池化层，然后使用了一个 $1 \times 1$ 的卷积层来改变通道数。最后四个通道通过通道合并层进行连接，构成整个 Inception 块的输出。

```python
class Inception(nn.Module):
    # c1--c4是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1x1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1x1卷积层后接3x3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1x1卷积层后接5x5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3x3最大汇聚层后接1x1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # 在通道维度上连结输出
        return torch.cat((p1, p2, p3, p4), dim=1)
```

随着网络层数的增多，性能应当越来越好，至少不应当比浅层的网络差，因为最糟的情况就是后面的层学习学习到的是恒等映射，性能与浅层网络相当，而不会更差，但是实际越与此相反，随着网络层数的增加，性能反而下降。根据此想法，何凯明等人提出了 ResNet 很好的解决了网络的深度的问题，ResNet 的核心是残差块。

残差块由两个 $3 \times 3$ 的卷积层组成，每个卷积层之后都应用了 ReLU 激活函数，除此之外，我们在输入与输出之间引入了一条“高速公路”，可以加快模型的收敛速率，考虑到卷积层可能改变输入特征的通道大小，为了能使得输出特征与输入特征进行相加，需要对输入特征投影，$1 \times 1$的卷积层只改变特征的通道数，而不改变特征的大小，所以 $1 \times 1$ 的卷积层可以使得输入特征的通道与输出特征的通道数相同，从而实现相加。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/毕业论文-ResNet.36rv0eqmiiq0.webp" style="zoom: 50%;" />

```python
class Residual(nn.Module):
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

## 生成对抗网络

生成对抗网络由 Goodfellow 等人于 2014 年提出，不同于一般的卷积神经网络，生成对抗网络的目的是学习出目标对象的数据分布，它包含两个模块，生成器和判别器，其总体模型架构如下所示。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/开题报告-生成对抗网络.20ejzzr9xhxc.webp" style="zoom: 50%;" />

生成器的作用是根据输入的确定的数据分布，通过数据的学习，将确定的数据分布映射到真实的数据分布，一般输入的随机噪声符合高斯分布或者均匀分布，加入随机噪声作为输入是为了控制输出的多样性，生成器的输出是符合真实数据分布的图像。判别器的作用是判别输入的图像的真假，生成器生成的图像是假图像，实际用于训练的目标图像为真图像。

判别器的输出是输入图像为真实图像的概率，生成器的目的是使其生成的图像通过判别器时，输出的概率值尽可能的大，而判别器的职责是要尽可能地分辨率出输入图像的真假，二者之间的目标是矛盾的，可以将生成器和判别器比喻为假币机和验钞机，假币机希望它造出的假币能骗过验钞机，而验钞机则要尽最大可能的分辨真假，正是这种对抗的关系，这种网络被称为生成对抗网络。二者在对抗中不断的进化，当判别器无法区分真假时，说明生成器生成的图像可以以假乱真，也就是说生成器学习到了目标图像的数据分布，训练的目的就达成了。 

<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px;">
<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/1.1fxuiuckbhls.webp" style="zoom: 50%;" />
<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/2.brb9w1w5e74.webp" />
<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/3.1mtad4z301r4.webp" />
</div>

可以前往 [这个网站](https://thispersondoesnotexist.com) 查看。

上图描述的是原始的生成对抗网络，使用随机噪声作为输入，使用随机噪声会导致生成的图片由噪声控制，这个是不可控的，图像生成过于自由，必须添加条件加以限制，使得输出可以控制，这需要为生成器的输入添加额外的条件，这便是条件生成对抗网络。

<img src="https://cdn.staticaly.com/gh/LastKnightCoder/ImgHosting3@master/开题报告-条件生成对抗网络.3ttwv8oqt2c0.webp" style="zoom: 50%;" />

[pix2pix](https://phillipi.github.io/pix2pix/) 使用条件生成对抗网络(cGAN)来实现图片到图片的转换

<img src="https://cdn.jsdelivr.net/gh/LastKnightCoder/image-for-2022@master/image-20221120152059469.png" alt="image-20221120152059469" style="zoom:50%;" />

[CycleGAN](https://junyanz.github.io/CycleGAN/) 使用 cGAN 实现图像之间的互相转换

<img src="https://cdn.jsdelivr.net/gh/LastKnightCoder/image-for-2022@master/image-20221120152342987.png" alt="image-20221120152342987" style="zoom:50%;" />

我的毕业论文[基于深度学习的毫米波图像处理研究](https://lastknightcoder.github.io/slides-dissertation/1) 也用到了 GAN，来对图像进行去噪处理，欢迎围观。

## 补充说明

一些说明：

1. 讲的都是比较老的技术，2015 年，现在大家都用 Transformer 了
2. 讲的都是图像处理的工作，还有很多自然语言处理的工作，比如机器翻译，情感分析等等
3. 讲的都是监督学习的工作，现在大家也在努力探索无监督学习，弱监督学习，小数据学习
4. 还有很多有趣的工作，比如最近多模态的工作也比较火，同时使用文本、声音、图像训练，比如，CLIP、Stable Diffusion

从一方面看，深度学习就像是一个黑盒，你喂给它数据，然后它自动的学习，但是你不知道它学到了什么，很多时候我们也无法从中获取到什么知识，从这个角度看，深度学习很难发展起来，因为我们甚至都无法控制它。从另一方面看，虽然我们不知道黑盒中发生了什么，但是它的确是一个非常神奇的技术，经常会给我们惊喜，在某些方面已经远超传统的算法，甚至逼近人类，比如围棋，图像分类，物体识别等等，并且每隔几年都会有一个比较好玩的东西出现，从这方面看，我觉得深度学习蛮有潜力的。

## 附录：毕业论文代码

```python
import torch
from torch import nn
import os
import cv2 as cv
import os.path as path
from torch.nn import functional as F
from torch.utils.data import Dataset
from torchvision import transforms
import time
from torch.autograd import Variable
from torch import autograd

class ImageDataset(Dataset):
    def __init__(self, origin_root, train_root):
        super().__init__()
        self.origin_root = origin_root
        self.train_root = train_root
        self.origin_files = os.listdir(self.origin_root)
        self.train_files = os.listdir(self.train_root)

    def __len__(self):
        return len(self.origin_files)

    def __getitem__(self, index):
        origin_data = cv.imread(path.join(self.origin_root, self.origin_files[index]))
        train_data = cv.imread(path.join(self.train_root, self.train_files[index]))
        to_tensor = transforms.ToTensor()
        origin_data = to_tensor(origin_data)[0].reshape((1, 678, 384))
        return origin_data, to_tensor(train_data)[0].reshape((1, 678, 384))

class ResNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResNetBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)

    def forward(self, x):
        if torch.any(torch.isnan(x)):
            torch.isnan(x)
        residual = x
        out = F.relu(self.conv1(x))
        out = self.conv2(out)
        if residual.size()[1] != out.size()[1]:
            residual = self.proj(residual)
        out += residual
        return F.relu(out)

class Generator(nn.Module):
    def __init__(self):
        super().__init__()

        self.model = nn.Sequential(
            # input is (1) x 678 x 384
            nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3),
            nn.InstanceNorm2d(64, affine=True),
            nn.ReLU(True),
            # state size. (64) x 678 x 384
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.InstanceNorm2d(128, affine=True),
            nn.ReLU(True),
            # state size. (128) x 339 x 192
            ResNetBlock(128, 128),
            ResNetBlock(128, 256, stride=2),
            ResNetBlock(256, 256),
            ResNetBlock(256, 512, stride=2),
            # state size. (512) x 85 x 48
            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(256, affine=True),
            nn.ReLU(True),
            # state size. (256) x 170 x 96
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(128, affine=True),
            nn.ReLU(True),
            # state size. (128) x 340 x 192
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(64, affine=True),
            nn.ReLU(True),
            # state size. (64) x 680 x 384
            nn.ConvTranspose2d(64, 1, kernel_size=7, stride=1, padding=(4, 3)),
            nn.Tanh()
            # output is (1) x 678 x 384
        )

    def forward(self, inputs):
        return self.model(inputs) + inputs

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.model = nn.Sequential(
            # input is (1) x 678 x 384
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
            nn.InstanceNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64) x 178 x 89
            nn.Conv2d(64, 128, kernel_size=3, stride=2),
            nn.InstanceNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (128) x 89 x 44
            nn.Conv2d(128, 256, kernel_size=3, stride=2),
            nn.InstanceNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=3, stride=2),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1),
        )

    def forward(self, x):
        x = self.model(x)
        return x
      
generator_model_name = 'wgan/wgan-generator.pth'
discriminator_model_name = 'wgan/wgan-discriminator.pth'
# 从上次训练结束点开始训练
start_point = 0

class WGAN(object):
    def __init__(self):
        super(WGAN, self).__init__()
        print("WGAN_GradientPenalty init model.")

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.G = Generator().to(self.device)
        self.D = Discriminator().to(self.device)
        self.batch_size = 16
        self.generator_iters = 400
        self.critic_iter = 3
        self.g_optimizer = torch.optim.Adam(self.G.parameters(), lr=1e-4, betas=(0.5, 0.999))
        self.d_optimizer = torch.optim.Adam(self.D.parameters(), lr=1e-4, betas=(0.5, 0.999))
        self.lambda_term = 10
        
        self.loss_function = nn.MSELoss()

        self.one = torch.tensor(1, dtype=torch.float).to(self.device)
        self.mone = (self.one * -1).to(self.device)

        dataset = ImageDataset(origin_root='dataset/train/images',
                               train_root='dataset/train/labels')
        self.data_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)
        self.data = self.get_infinite_batches()

        self.init_model()

    def init_model(self):
        if os.path.exists(generator_model_name) and os.path.exists(discriminator_model_name):
            self.G.load_state_dict(torch.load(generator_model_name))
            self.D.load_state_dict(torch.load(discriminator_model_name))
            print(f'Models load from {generator_model_name} & {discriminator_model_name}')
        else:
            print('No trained_models found, init new trained_models')
            self.G.apply(self.weights_init)
            self.D.apply(self.weights_init)

    def weights_init(self, m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            nn.init.xavier_normal_(m.weight.data)
            m.bias.data.fill_(0)

    def train(self):
        for epoch in range(self.generator_iters):
            begin = time.time()
            print(f'Epoch: {epoch + 1 + start_point} / {self.generator_iters + start_point} ============================')
            # 训练判别器
            for p in self.D.parameters():
                p.requires_grad = True

            d_loss=0
            Wasserstein_D=0
            # 训练 3 次判别器，训练 1 次生成器
            for d_iter in range(self.critic_iter):
                self.D.zero_grad()
                inputs, reals = next(self.data)
                
                if inputs.size()[0] != self.batch_size:
                    continue

                d_loss_real = self.D(reals).mean()
                d_loss_real.backward(self.mone)

                fakes = self.G(inputs)
                d_loss_fake = self.D(fakes).mean()
                d_loss_fake.backward(self.one)

                gradient_penalty = self.calculate_gradient_penalty(reals.data, fakes.data)
                gradient_penalty.backward()

                d_loss = d_loss_fake - d_loss_real + gradient_penalty
                Wasserstein_D = d_loss_real - d_loss_fake
                self.d_optimizer.step()
                
            # 固定判别器，训练生成器
            for p in self.D.parameters():
                p.requires_grad = False

            self.G.zero_grad()
            inputs, reals = next(self.data)
            fakes = self.G(inputs)
            g_loss = self.D(fakes).mean()
            g_loss.backward(self.mone)
            g_cost = -g_loss 
            
            self.g_optimizer.step()
            print(f'D loss: {d_loss.item()}, G loss: {g_loss.item()}, Wasserstein_D: {Wasserstein_D.item()}, cost time: {time.time() - begin}')
            self.save_model()
            
            if (epoch + 1) % 50 == 0:
                torch.save(self.G.state_dict(), f'wgan/wgan-generator-{epoch + start_point + 1}.pth')
                torch.save(self.D.state_dict(), f'wgan/wgan-discriminator-{epoch + start_point + 1}.pth')

    def get_infinite_batches(self):
        while True:
            for i, (inputs, reals) in enumerate(self.data_loader):
                inputs = inputs.to(self.device)
                reals = reals.to(self.device)
                yield inputs, reals

    def calculate_gradient_penalty(self, real_images, fake_images):
        eta = torch.FloatTensor(self.batch_size, 1, 1, 1).uniform_(0, 1)
        eta = eta.expand(self.batch_size, real_images.size(1), real_images.size(2), real_images.size(3))
        eta = eta.to(self.device)

        interpolated = eta * real_images + ((1 - eta) * fake_images)
        interpolated = interpolated.to(self.device)

        # define it to calculate gradient
        interpolated = Variable(interpolated, requires_grad=True)

        # calculate probability of interpolated examples
        prob_interpolated = self.D(interpolated)

        # calculate gradients of probabilities with respect to examples
        gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,
                                  grad_outputs=torch.ones(
                                      prob_interpolated.size()).cuda(0),
                                  create_graph=True, retain_graph=True)[0]

        grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lambda_term
        return grad_penalty

    def save_model(self):
        torch.save(self.G.state_dict(), generator_model_name)
        torch.save(self.D.state_dict(), discriminator_model_name)
        print(f'Models save to {generator_model_name} & {discriminator_model_name}')

model = WGAN()
model.train()
```

